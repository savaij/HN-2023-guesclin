@inproceedings{camps2021,
  title = {Handling {{Heavily Abbreviated Manuscripts}}: {{HTR}} engines vs text normalisation approaches},
  shorttitle = {Handling {{Heavily Abbreviated Manuscripts}}},
  booktitle = {International {{Conference}} on {{Document Analysis}} and {{Recognition}} 2021},
  author = {Camps, Jean-Baptiste and {Vidal-Gorène}, Chahan and Vernet, Marguerite},
  year = {2021},
  pages = {306},
  doi = {10.1007/978-3-030-86159-9_21},
  url = {https://enc.hal.science/hal-03279602},
  urldate = {2024-03-28},
  abstract = {Although abbreviations are fairly common in handwritten sources, particularly in medieval and modern Western manuscripts, previous research dealing with computational approaches to their expansion is scarce. Yet abbreviations present particular challenges to computational approaches such as handwritten text recognition and natural language processing tasks. Often, pre-processing ultimately aims to lead from a digitised image of the source to a normalised text, which includes expansion of the abbreviations. We explore different setups to obtain such a normalised text, either directly, by training HTR engines on normalised (i.e., expanded, disabbreviated) text, or by decomposing the process into discrete steps, each making use of specialist models for recognition, word segmentation and normalisation. The case studies considered here are drawn from the medieval Latin tradition.},
  langid = {english},
  file = {/Users/sava/Zotero/storage/8K4LBQA9/Camps et al. - 2021 - Handling Heavily Abbreviated Manuscripts HTR engi.pdf}
}

@inproceedings{clerice2022,
  title = {Ground-truth {{Free Evaluation}} of {{HTR}} on {{Old French}} and {{Latin Medieval Literary Manuscripts}}},
  booktitle = {Workshop on {{Computational Humanities Research}}},
  author = {Clérice, Thibault},
  year = {2022},
  url = {https://www.semanticscholar.org/paper/Ground-truth-Free-Evaluation-of-HTR-on-Old-French-Cl%C3%A9rice/8d34d21cd69a2a49b0be94795d384e79972fec19},
  urldate = {2024-03-28},
  abstract = {As more and more projects openly release ground truth for handwritten text recognition (HTR), we expect the quality of automatic transcription to improve on unseen data. Getting models robust to scribal and material changes is a necessary step for speci昀椀c data mining tasks. However, evaluation of HTR results requires ground truth to compare prediction statistically. In the context of modern languages, successful attempts to evaluate quality have been done using lexical features or n-grams.This, however, proves di昀케cult in the context of spelling variation that both Old French and Latin have, even more so in the context of sometime heavily abbreviated manuscripts. We propose a new method based on deep learning where we attempt to categorize each line error rate into four error rate ranges (},
  file = {/Users/sava/Zotero/storage/XMCKJLRS/Clérice - 2022 - Ground-truth Free Evaluation of HTR on Old French .pdf},
  language = {english}
}

@misc{gueville2023,
  title = {Transcribing {{Medieval Manuscripts}} for {{Machine Learning}}},
  author = {Guéville, Estelle and Wrisley, David Joseph},
  year = {2023},
  month = oct,
  eprint = {2207.07726},
  primaryclass = {cs},
  doi = {10.46298/jdmdh.9805},
  url = {http://arxiv.org/abs/2207.07726},
  urldate = {2024-03-28},
  abstract = {This article focuses on the transcription of medieval manuscripts. Whereas problems of transcription have long interested medievalists, few workable options in the era of printed editions were available besides normalisation. The automation of this process, known as handwritten text recognition (HTR), has made new kinds of digital text creation possible, but also has foregrounded the necessity of theorising transcription in our scholarly practices. We reflect here on different notions of transcription against the backdrop of changing text technologies. Moreover, drawing on our own research on medieval Latin Bibles, we present general guidelines for customizing transcription schemes, arguing that they must be designed with specific research questions and scholarly end use in mind. Since we are particularly interested in the scribal contribution to the production of codices, our transcription guidelines aim to capture abbreviations and orthographic variation between different textual witnesses for downstream machine learning tasks. In the final section of the article, we discuss a few examples of how the HTR-created transcriptions allow us to address new questions at scale in medieval manuscripts, such as textual variance across witnesses, the prediction of a change in scribal hands within a single manuscript as well as the profiling of individual and regional scribal characteristics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Digital Libraries},
  file = {/Users/sava/Zotero/storage/7BVCVSXR/Guéville e Wrisley - 2023 - Transcribing Medieval Manuscripts for Machine Lear.pdf;/Users/sava/Zotero/storage/CG33FKJI/2207.html},
  language = {english}
}

@unpublished{pinche2022,
  title = {Guide de transcription pour les manuscrits du {{Xe}} au {{XVe}} siècle},
  author = {Pinche, Ariane},
  year = {2022},
  month = jun,
  url = {https://hal.science/hal-03697382},
  urldate = {2024-03-28},
  abstract = {Ce guide est le fruit d’une réflexion menée au cours des années 2021 et 2022 dans la cadre du projet CREMMALab et du séminaire de recherche « Création de modèle(s) HTR pour les documents médiévaux en ancien français et moyen français entre le Xe et le XVe siècle ». Nos préconisations ont pour but d’accompagner la création des données d’entraînement afin d’optimiser l’apprentissage machine des modèles d’HTR. Produire des données d’entraînement demande de modéliser une représentation la matière textuelle afin de créer des données homogènes qui assureront la qualité des prédictions HTR. Ce guide ne prétend pas répondre à toutes les problématiques de transcriptions présentes dans les manuscrits médiévaux, mais nous espérons qu’il permettra aux transcripteurs de répondre aux interrogations les plus courantes et d’harmoniser les pratiques.},
  file = {/Users/sava/Zotero/storage/HUPAVT9I/Pinche - 2022 - Guide de transcription pour les manuscrits du Xe a.pdf},
  language = {french}
}

@article{schoen2022,
  title = {Optical {{Character Recognition}} ({{OCR}}) and {{Medieval Manuscripts}}: {{Reconsidering Transcriptions}} in the {{Digital Age}}},
  shorttitle = {Optical {{Character Recognition}} ({{OCR}}) and {{Medieval Manuscripts}}},
  author = {Schoen, Jenna and Saretto, Gianmarco E.},
  year = {2022},
  journal = {Digital Philology: A Journal of Medieval Cultures},
  volume = {11},
  number = {1},
  pages = {174--206},
  publisher = {{Johns Hopkins University Press}},
  issn = {2162-9552},
  url = {https://muse.jhu.edu/pub/1/article/853521},
  urldate = {2024-03-28},
  abstract = {This essay will discuss an ongoing project to train an optical character recognition (OCR) system on medieval manuscripts—specifically, the OCR engine Kraken, which we trained to transcribe early-fifteenth-century Middle English manuscripts. Our current model, trained on Scribe D's handwriting, has a 97 percent training accuracy rate and transcribes unseen manuscripts with a range of accuracy rates between 27 and 86 percent. Our project adds to the growing number of successful experiments in training medieval manuscripts on OCR, a technology that could have an immense impact on medieval studies., The primary concern of this essay is not our specific results but the challenges we faced when preparing our training data and the decisions we made accordingly. In particular, we compare the diplomatic transcriptions required by our software to the semi-diplomatic transcriptions that medievalists usually create. We argue that technical constraints such as the use of diplomatic transcriptions in OCR might encourage medievalists to evaluate how we typically remediate manuscripts (that is, transfer them from one medium to another). Considering the potential scope and scalability of this technology, we argue that it is important to consider our training data (human-made transcriptions) carefully, as is the case for any machinelearning project. But we also argue that machine learning offers a useful framework for understanding how we manipulate manuscript data in any kind of remediation.},
  language = {english}
}

@misc{zotero-151,
  title = {cremma-medieval/htr-united.yml at main · {{HTR-United}}/cremma-medieval},
  journal = {GitHub},
  url = {https://github.com/HTR-United/cremma-medieval/blob/main/htr-united.yml},
  urldate = {2024-03-28},
  abstract = {Transcription corpora for training HTR models for medieval manuscripts from the 12th to the 15th century. - HTR-United/cremma-medieval},
  langid = {english},
  file = {/Users/sava/Zotero/storage/CVCTKYAR/htr-united.html},
  language = {english}
}

@misc{zotero-153,
  title = {{Compte-rendu de la séance n° 3}},
  journal = {CREMMALAB},
  url = {https://cremmalab.hypotheses.org/compte-rendu-seance-3},
  urldate = {2024-03-28},
  abstract = {L’allographie, entre besoins scientifiques et pragmatiques. Comment modéliser et optimiser les données d’entraînement pour l’HTR (I) ? Mardi 14 décembre — 16h-18h En présence de Vera Schwarz-Ricci, Jean-Baptiste Camps, Camille Carnaille, Prunelle Deleville, Lucien Dugaz, Frédéric Duval, Lucence Ing, Vincent Jolivet, Marco Maulu, Viola Mariotti , Ariane Pinche, Anne Rochebouet, Aurélia Rostaing, Benedetta Salvati, Peter […]},
  langid = {french},
  file = {/Users/sava/Zotero/storage/JLFH3BT8/compte-rendu-seance-3.html},
  language = {french}
}
